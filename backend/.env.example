# Live Stream Chat AI Agent Backend Configuration (.env.example)
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments.

# --- Core Server Settings ---
# SERVER_HOST: IP address to bind the server to.
#   '0.0.0.0' makes it accessible from other machines on the network.
#   '127.0.0.1' restricts access to the local machine only.
SERVER_HOST=0.0.0.0

# SERVER_PORT: Port number the server will listen on.
SERVER_PORT=8181

# SERVER_ENABLE_SSL: Enable HTTPS. Set to 'true' to enable, 'false' to disable (HTTP).
# If true, SSL_CERT_PATH and SSL_KEY_PATH must be valid.
SERVER_ENABLE_SSL=false

# SSL_CERT_PATH: Absolute or relative path to your SSL certificate file (e.g., fullchain.pem).
# Required if SERVER_ENABLE_SSL is true.
# Example: /etc/letsencrypt/live/yourdomain.com/fullchain.pem
SSL_CERT_PATH=

# SSL_KEY_PATH: Absolute or relative path to your SSL private key file (e.g., privkey.pem).
# Required if SERVER_ENABLE_SSL is true.
# Example: /etc/letsencrypt/live/yourdomain.com/privkey.pem
SSL_KEY_PATH=

# SERVER_TEST_MODE: Save incoming audio/screenshot and processing info for debugging.
# Set to 'true' to enable, 'false' to disable. Recommended: false for production.
SERVER_TEST_MODE=false

# --- LLM (Language Model) Configuration ---
# LLM_API_KEY: Your API key for the OpenAI-compatible service. (REQUIRED)
LLM_API_KEY=

# LLM_API_URL: The base URL of the OpenAI-compatible API endpoint. (REQUIRED)
# Example for OpenAI: https://api.openai.com/v1
# Example for local LM Studio: http://localhost:1234/v1
LLM_API_URL=https://api.openai.com/v1

# LLM_API_MODEL: The model identifier to use for chat completions (e.g., gpt-4o, gpt-4.5).
# Recommended model: claude-3-7-sonnet-20250219
LLM_API_MODEL=

# LLM_TOKENIZER_MODEL: The model name used *specifically* for calculating token counts with tiktoken.
# Often the same as LLM_API_MODEL, but sometimes a base model like 'gpt-4' works better for token estimation across variants.
# If you're not sure and your model is not in the tokenizer support list, you can try using gpt-4 or gpt-4o.
LLM_TOKENIZER_MODEL=gpt-4o

# LLM_MAX_RESPONSE_TOKENS: Maximum number of tokens the LLM is allowed to generate in a single response.
LLM_MAX_RESPONSE_TOKENS=2000

# LLM_API_TIMEOUT_SECONDS: Timeout duration (in seconds) for waiting for a response from the LLM API.
LLM_API_TIMEOUT_SECONDS=60

# --- Token Limits for Prompt Construction ---
# These control how much context (history, notepad, chat) is included in the prompt sent to the LLM.
# Tune these based on your LLM's context window size and cost considerations.

# PROMPT_MAX_TOTAL_TOKENS: The absolute maximum number of tokens allowed in the *entire* prompt + expected response buffer.
# This should be less than your model's context window limit (e.g., 4096, 8192, 128000).
# Includes system prompt, history, notepad, chat list, current input, and leaves space for the response.
PROMPT_MAX_TOTAL_TOKENS=4096

# PROMPT_MAX_NOTEPAD_TOKENS: Maximum tokens allocated for the {notepad: ...} section within the prompt.
PROMPT_MAX_NOTEPAD_TOKENS=712

# PROMPT_MAX_CHATLIST_TOKENS: Maximum tokens allowed for the {Chatlist content: ...} block when building the user's input message for this round.
PROMPT_MAX_CHATLIST_TOKENS=256

# --- Speech-to-Text (STT) Configuration ---
# STT_PROVIDER: Choose the preferred STT service. Options: 'youdao', 'whisper', 'both'.
#   'youdao': Use Youdao ASR (requires YOUDAO_* keys). Good for Chinese.
#   'whisper': Use Whisper via the LLM_API_URL endpoint (requires LLM_API_KEY). Good multilingual support.
#   'both': Use both and include both results in the prompt.
#   'compare': (Special mode set via command line, not here) Runs both but doesn't call LLM.
STT_PROVIDER=youdao

# YOUDAO_APP_KEY: Your Youdao Application Key (required if STT_PROVIDER includes 'youdao').
YOUDAO_APP_KEY=

# YOUDAO_APP_SECRET: Your Youdao Application Secret (required if STT_PROVIDER includes 'youdao').
YOUDAO_APP_SECRET=

# YOUDAO_API_URL: Youdao ASR API endpoint URL. (Default is usually fine).
YOUDAO_API_URL=https://openapi.youdao.com/asrapi

# --- Vision / Screenshot Settings ---
# VISION_ENABLE: Enable processing of screenshots. Set to 'true' to enable, 'false' to disable.
# If true, Cloudinary settings might be required depending on VISION_UPLOAD_PROVIDER.
VISION_ENABLE=false

# VISION_UPLOAD_PROVIDER: Where to upload screenshots. Options: 'cloudinary', 'none'.
#   'cloudinary': Upload to Cloudinary (Requires CLOUDINARY_* settings). Image URL is sent to LLM.
#   'none': Screenshots are received but not uploaded or sent to the LLM (useful for local testing/saving).
VISION_UPLOAD_PROVIDER=cloudinary

# CLOUDINARY_CLOUD_NAME: Your Cloudinary cloud name (required if VISION_UPLOAD_PROVIDER='cloudinary').
CLOUDINARY_CLOUD_NAME=

# CLOUDINARY_API_KEY: Your Cloudinary API key (required if VISION_UPLOAD_PROVIDER='cloudinary').
CLOUDINARY_API_KEY=

# CLOUDINARY_API_SECRET: Your Cloudinary API secret (required if VISION_UPLOAD_PROVIDER='cloudinary').
CLOUDINARY_API_SECRET=

# CLOUDINARY_UPLOAD_FOLDER: Folder name on Cloudinary to store uploaded screenshots.
CLOUDINARY_UPLOAD_FOLDER=live_screenshot

# IMAGE_COMPRESSION_QUALITY: JPEG quality for compressing screenshots before upload (1-95). Lower means smaller file, lower quality.
# Set to 0 or > 95 to disable compression (uploads original PNG/JPG).
IMAGE_COMPRESSION_QUALITY=60

# --- System Prompt Configuration ---
# SYSTEM_PROMPT_MODE: How the system prompt is handled. Options: 'standard', 'user_message_compatibility'.
#   'standard': Send prompt using the 'system' role (Recommended for most models).
#   'user_message_compatibility': Send the system prompt as the *first* 'user' message (for models that don't support the system role well).
SYSTEM_PROMPT_MODE=standard

# SYSTEM_PROMPT_PATH: Path to a file containing the system prompt text. UTF-8 encoding is expected.
# If empty or file not found, a built-in default prompt will be used.
# Example: ./prompts/system_prompt_龟背竹.txt
SYSTEM_PROMPT_PATH=

# --- File Paths and Prefixes ---
# MEMORY_BASE_DIR: Directory where persistent data (notepad, context) for each room is stored.
MEMORY_BASE_DIR=memory

# TEST_FILES_DIR: Directory where files are saved when SERVER_TEST_MODE is true.
TEST_FILES_DIR=test

# AUDIO_TEMP_PREFIX: Prefix for temporary audio files created during processing.
AUDIO_TEMP_PREFIX=live_audio_

# SCREENSHOT_TEMP_PREFIX: Prefix for temporary screenshot files created during processing.
SCREENSHOT_TEMP_PREFIX=live_screenshot_

# FFMPEG_PATH: Path to the FFmpeg executable. Used for audio conversion (e.g., for Youdao).
# If ffmpeg is in your system's PATH, just 'ffmpeg' is usually sufficient.
FFMPEG_PATH=ffmpeg
